{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS634_Project1_Team3",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtikyani/CS634_IBM_Fairness_Pipeline/blob/master/CS634_Project1_Team3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMzUNRDqsmn3",
        "colab_type": "text"
      },
      "source": [
        "# Step 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y91oeSQQliC5",
        "colab_type": "text"
      },
      "source": [
        "Read the paper and fully understand the fairness pipeline. Write your own tutorial summary of the pipeline in a markdown file. (25 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7pFiVO2lqdl",
        "colab_type": "text"
      },
      "source": [
        "Summary of AI Fairness 360 \n",
        "\n",
        "\n",
        "As machine learning models become more widely used in domains such as mortgage lending, hiring and prison sentencing, fairness and bias are becoming central foci of the algorithms used in these models. To address issues related to fairness, AI Fairness 360 (AIF360) was developed to help better understand fairness metrics and mitigation techniques.\n",
        "\n",
        "favorable label: value corresponds to an advantageous outcome\n",
        "protected attribute: partitions a population into groups with different amounts of benefits, ie) race, gender, caste, religion\n",
        "privileged: group that has historically been at a systematic advantage\n",
        "group fairness: groups defined by protected attributes receive similar treatments or outcomes\n",
        "individual fairness: similar individuals receiving similar treatments or outcomes\n",
        "bias: systematic error that gives privileged groups advantages and vice versa for unprivileged groups\n",
        "fairness metric: quantification of unwanted bias in training data or models\n",
        "bias mitigation algorithm: used to reduce unwanted bias in training data or models\n",
        "\n",
        "Many other toolkits exist, although they are not as comprehensive as AIF360 in that AIF 360 provides bisa metrics, mitigation algorithms, bias metric explanations, and industrial usability. \n",
        "\n",
        "AIF 360 is an end-to-end workflow that allows users to transform raw data to a model, adding in functionality throughout. The general pipeline for bias mitigation works as follows: outputs are new datasets sharing the same protected attributes as other datasets, transitions are transformations that modify features and/or labels, and trapezoids are learned models. One can reach a fair prediction through one of three bias mitigation algorithms: fair pre-processing, fair in-processing, fair post-processing. The three methods each have their nuances, so one must decide which they would like to use based on its respective features.\n",
        "\n",
        "The Dataset class handles all forms of data (like training data and testing data), and organizes the data by associating each instance of data with protected attributes. It also contains subclasses that are more specific and further organize the data. These classes serve to provide a common structure for the entire pipeline to use and also allow different utility functions and capabilities (from Python) to be applied to the data. \n",
        "\n",
        "The Metrics class and its subclasses calculate fairness matrics and check for biases in datasets and models. Because fairness can be defined in many ways and is dependent on the context, many different fairness metrics are included in the toolkit, which gives the user the option to select the best metrics given the data. \n",
        "\n",
        "The Explainer class is intended to be associated with the Metrics class.  It is used to provide further insights on computed fairness metrics.  The Explainer class stresses the need for explanations.  Future methodologies may include actionable recourse analysis, counterfactual fairness, and fine-grained localization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v0PzTiEmsuNc"
      },
      "source": [
        "# Step 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdfGiSfElzTY",
        "colab_type": "text"
      },
      "source": [
        "Execute the credit decision pipeline that is detecting age bias and removing using the reweighting algorithm and explain the findings. (25 points) -- Executed Below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "donO-W34jFMe",
        "colab_type": "code",
        "outputId": "38bee3c9-f272-4daa-f7a1-3e98d77c72db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "\n",
        "#!pip install aif360\n",
        "#!pip install IPython\n",
        "\n",
        "# Load all necessary packages\n",
        "import sys\n",
        "sys.path.insert(1, \"../\")  \n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from aif360.datasets import GermanDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "dataset_orig = GermanDataset(\n",
        "    protected_attribute_names=['age'],           # this dataset also contains protected\n",
        "                                                 # attribute for \"sex\" which we do not\n",
        "                                                 # consider in this evaluation\n",
        "    privileged_classes=[lambda x: x >= 25],      # age >=25 is considered privileged\n",
        "    features_to_drop=['personal_status', 'sex'] # ignore sex-related attributes\n",
        ")\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "\n",
        "privileged_groups = [{'age': 1}]\n",
        "unprivileged_groups = [{'age': 0}]\n",
        "\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
        "\n",
        "#USING REWEIGHTING ALGORITHM\n",
        "\n",
        "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                privileged_groups=privileged_groups)\n",
        "dataset_transf_train = RW.fit_transform(dataset_orig_train)\n",
        "\n",
        "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
        "                                               unprivileged_groups=unprivileged_groups,\n",
        "                                               privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Transformed training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Original training dataset",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Transformed training dataset",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ootwdNjRl5tK",
        "colab_type": "text"
      },
      "source": [
        "[OUTPUT]:\n",
        "Original training dataset:\n",
        "Difference in mean outcomes between unprivileged and privileged groups = -0.169905\n",
        "Transformed training dataset:\n",
        "Difference in mean outcomes between unprivileged and privileged groups = 0.000000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KslImDfJznGh",
        "colab_type": "text"
      },
      "source": [
        "Findings:\n",
        "Protected Attribute: Age\n",
        "Attributes not accounted for: Sex & Personal Status\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wYrz8pjis1RK"
      },
      "source": [
        "# Step 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgBDwnkVmBJ5",
        "colab_type": "text"
      },
      "source": [
        "Apply a different method for detecting and removing bias. (25 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e09gDwTqmHsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Need to apply a different method than the Reweighting Function which is built-in \n",
        "\n",
        "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                privileged_groups=privileged_groups)\n",
        "dataset_transf_train = RW.fit_transform(dataset_orig_train)\n",
        "\n",
        "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
        "                                               unprivileged_groups=unprivileged_groups,\n",
        "                                               privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Transformed training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUn19Nj8Bzsz",
        "colab_type": "text"
      },
      "source": [
        "Reweighing.py Source Code Below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ceq6h4NSBueZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from aif360.algorithms import Transformer\n",
        "from aif360.metrics import utils\n",
        "\n",
        "\n",
        "class Reweighing(Transformer):\n",
        "    \"\"\"Reweighing is a preprocessing technique that Weights the examples in each\n",
        "    (group, label) combination differently to ensure fairness before\n",
        "    classification [4]_.\n",
        "    References:\n",
        "        .. [4] F. Kamiran and T. Calders,  \"Data Preprocessing Techniques for\n",
        "           Classification without Discrimination,\" Knowledge and Information\n",
        "           Systems, 2012.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, unprivileged_groups, privileged_groups):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            unprivileged_groups (list(dict)): Representation for unprivileged\n",
        "                group.\n",
        "            privileged_groups (list(dict)): Representation for privileged group.\n",
        "        \"\"\"\n",
        "        super(Reweighing, self).__init__(\n",
        "            unprivileged_groups=unprivileged_groups,\n",
        "            privileged_groups=privileged_groups)\n",
        "\n",
        "        self.unprivileged_groups = unprivileged_groups\n",
        "        self.privileged_groups = privileged_groups\n",
        "\n",
        "        self.w_p_fav = 1.\n",
        "        self.w_p_unfav = 1.\n",
        "        self.w_up_fav = 1.\n",
        "        self.w_up_unfav = 1.\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        \"\"\"Compute the weights for reweighing the dataset.\n",
        "        Args:\n",
        "            dataset (BinaryLabelDataset): Dataset containing true labels.\n",
        "        Returns:\n",
        "            Reweighing: Returns self.\n",
        "        \"\"\"\n",
        "\n",
        "        (priv_cond, unpriv_cond, fav_cond, unfav_cond,\n",
        "        cond_p_fav, cond_p_unfav, cond_up_fav, cond_up_unfav) =\\\n",
        "                self._obtain_conditionings(dataset)\n",
        "\n",
        "        n = np.sum(dataset.instance_weights, dtype=np.float64)\n",
        "        n_p = np.sum(dataset.instance_weights[priv_cond], dtype=np.float64)\n",
        "        n_up = np.sum(dataset.instance_weights[unpriv_cond], dtype=np.float64)\n",
        "        n_fav = np.sum(dataset.instance_weights[fav_cond], dtype=np.float64)\n",
        "        n_unfav = np.sum(dataset.instance_weights[unfav_cond], dtype=np.float64)\n",
        "\n",
        "        n_p_fav = np.sum(dataset.instance_weights[cond_p_fav], dtype=np.float64)\n",
        "        n_p_unfav = np.sum(dataset.instance_weights[cond_p_unfav],\n",
        "                           dtype=np.float64)\n",
        "        n_up_fav = np.sum(dataset.instance_weights[cond_up_fav],\n",
        "                          dtype=np.float64)\n",
        "        n_up_unfav = np.sum(dataset.instance_weights[cond_up_unfav],\n",
        "                            dtype=np.float64)\n",
        "\n",
        "        # reweighing weights\n",
        "        self.w_p_fav = n_fav*n_p / (n*n_p_fav)\n",
        "        self.w_p_unfav = n_unfav*n_p / (n*n_p_unfav)\n",
        "        self.w_up_fav = n_fav*n_up / (n*n_up_fav)\n",
        "        self.w_up_unfav = n_unfav*n_up / (n*n_up_unfav)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        \"\"\"Transform the dataset to a new dataset based on the estimated\n",
        "        transformation.\n",
        "        Args:\n",
        "            dataset (BinaryLabelDataset): Dataset that needs to be transformed.\n",
        "        Returns:\n",
        "            dataset (BinaryLabelDataset): Dataset with transformed\n",
        "                instance_weights attribute.\n",
        "        \"\"\"\n",
        "\n",
        "        dataset_transformed = dataset.copy(deepcopy=True)\n",
        "\n",
        "        (_, _, _, _, cond_p_fav, cond_p_unfav, cond_up_fav, cond_up_unfav) =\\\n",
        "                            self._obtain_conditionings(dataset)\n",
        "\n",
        "        # apply reweighing\n",
        "        dataset_transformed.instance_weights[cond_p_fav] *= self.w_p_fav\n",
        "        dataset_transformed.instance_weights[cond_p_unfav] *= self.w_p_unfav\n",
        "        dataset_transformed.instance_weights[cond_up_fav] *= self.w_up_fav\n",
        "        dataset_transformed.instance_weights[cond_up_unfav] *= self.w_up_unfav\n",
        "\n",
        "        return dataset_transformed\n",
        "\n",
        "##############################\n",
        "#### Supporting functions ####\n",
        "##############################\n",
        "    def _obtain_conditionings(self, dataset):\n",
        "        \"\"\"Obtain the necessary conditioning boolean vectors to compute\n",
        "        instance level weights.\n",
        "        \"\"\"\n",
        "        # conditioning\n",
        "        priv_cond = utils.compute_boolean_conditioning_vector(\n",
        "                            dataset.protected_attributes,\n",
        "                            dataset.protected_attribute_names,\n",
        "                            condition=self.privileged_groups)\n",
        "        unpriv_cond = utils.compute_boolean_conditioning_vector(\n",
        "                            dataset.protected_attributes,\n",
        "                            dataset.protected_attribute_names,\n",
        "                            condition=self.unprivileged_groups)\n",
        "        fav_cond = dataset.labels.ravel() == dataset.favorable_label\n",
        "        unfav_cond = dataset.labels.ravel() == dataset.unfavorable_label\n",
        "\n",
        "        # combination of label and privileged/unpriv. groups\n",
        "        cond_p_fav = np.logical_and(fav_cond, priv_cond)\n",
        "        cond_p_unfav = np.logical_and(unfav_cond, priv_cond)\n",
        "        cond_up_fav = np.logical_and(fav_cond, unpriv_cond)\n",
        "        cond_up_unfav = np.logical_and(unfav_cond, unpriv_cond)\n",
        "\n",
        "        return (priv_cond, unpriv_cond, fav_cond, unfav_cond,\n",
        "            cond_p_fav, cond_p_unfav, cond_up_fav, cond_up_unfav)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hfEBpuDIUDY",
        "colab_type": "code",
        "outputId": "1802f95a-2f30-4d47-9c66-3fab6ecbf88a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "\n",
        "#!pip install aif360\n",
        "#!pip install IPython\n",
        "#!pip install BlackBoxAuditing\n",
        "\n",
        "# Load all necessary packages\n",
        "import sys\n",
        "sys.path.insert(1, \"../\")  \n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from aif360.datasets import GermanDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.algorithms.preprocessing import LFR\n",
        "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
        "\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "dataset_orig = GermanDataset(\n",
        "    protected_attribute_names=['age'],           # this dataset also contains protected\n",
        "                                                 # attribute for \"sex\" which we do not\n",
        "                                                 # consider in this evaluation\n",
        "    privileged_classes=[lambda x: x >= 25],      # age >=25 is considered privileged\n",
        "    features_to_drop=['personal_status', 'sex'] # ignore sex-related attributes\n",
        ")\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "\n",
        "privileged_groups = [{'age': 1}]\n",
        "unprivileged_groups = [{'age': 0}]\n",
        "\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
        "\n",
        "#USING REWEIGHTING ALGORITHMS\n",
        "#\n",
        "#RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "#                privileged_groups=privileged_groups)\n",
        "#dataset_transf_train = RW.fit_transform(dataset_orig_train)\n",
        "#\n",
        "#metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
        "#                                               unprivileged_groups=unprivileged_groups,\n",
        "#                                               privileged_groups=privileged_groups)\n",
        "#display(Markdown(\"#### Transformed training dataset\"))\n",
        "#print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#Disparate Impact Remover PreProcessor Method  \n",
        "DIR = DisparateImpactRemover(repair_level = 1.0)\n",
        "\n",
        "dataset_transf_DIR = DIR.fit_transform(dataset_orig_train)\n",
        "dataset_transf_DIR_train, dataset_transf_DR_test, = dataset_transf_DIR.split([0.7], shuffle=True)\n",
        "\n",
        "metric_DIR_train = BinaryLabelDatasetMetric(dataset_transf_train, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "display(Markdown(\"#### Transformed training dataset using Disparate Impact Remover PreProcessor Method\"))\n",
        "\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_DIR_train.mean_difference())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Original training dataset",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Difference in mean outcomes between unprivileged and privileged groups = -0.169905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Transformed training dataset using Disparate Impact Remover PreProcessor Method",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Id2EfMVhs__n"
      },
      "source": [
        "# Step 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwxnb5DotQRo",
        "colab_type": "text"
      },
      "source": [
        "Write a thorough explanation comparing the reweighting and the method you selected. (25 points)"
      ]
    }
  ]
}