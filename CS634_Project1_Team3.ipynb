{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS634_Project1_Team3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtikyani/CS634_IBM_Fairness_Pipeline/blob/master/CS634_Project1_Team3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMzUNRDqsmn3",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial summary of the pipeline in a markdown file:\n",
        "\n",
        "# [**Summary.md**](https://github.com/rtikyani/CS634_IBM_Fairness_Pipeline/blob/master/README.md)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v0PzTiEmsuNc"
      },
      "source": [
        "# Execute the credit decision pipeline that is detecting age bias and removing using the reweighting algorithm and explain the findings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "donO-W34jFMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Part 2\n",
        "\n",
        "#install all necessary libraries and modules\n",
        "!pip install aif360\n",
        "!pip install IPython\n",
        "!pip install BlackBoxAuditing\n",
        "!pip install wget\n",
        "\n",
        "\n",
        "#Retrieve Necessary Files \n",
        "import os\n",
        "os.chdir('/usr/local/lib/python3.6/dist-packages/aif360/data/raw/german')\n",
        "!wget -q\thttps://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
        "!wget -q\thttps://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc\n",
        "\n",
        "\n",
        "#import the Methods used from aif360\n",
        "from aif360.datasets import GermanDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "#Loading dataset\n",
        "dataset_orig = GermanDataset(\n",
        "    protected_attribute_names=['age'],           # this dataset also contains protected\n",
        "                                                 # attribute for \"sex\" which we do not\n",
        "                                                 # consider in this evaluation\n",
        "    privileged_classes=[lambda x: x >= 25],      # age >=25 is considered privileged\n",
        "    features_to_drop=['personal_status', 'sex'] # ignore sex-related attributes\n",
        ")\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "privileged_groups = [{'age': 1}]\n",
        "unprivileged_groups = [{'age': 0}]\n",
        "\n",
        "#Metric for original data set\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
        "\n",
        "#Applying Reweighing Pre-Processing method to mitigate the bias\n",
        "\n",
        "#Train with and transform the original training data\n",
        "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                privileged_groups=privileged_groups)\n",
        "dataset_transf_train = RW.fit_transform(dataset_orig_train)\n",
        "\n",
        "# Metric with the transformed training data\n",
        "\n",
        "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
        "                                               unprivileged_groups=unprivileged_groups,\n",
        "                                               privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Transformed training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhqy7qZr3_27",
        "colab_type": "text"
      },
      "source": [
        "# Findings:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KslImDfJznGh",
        "colab_type": "text"
      },
      "source": [
        "[OUTPUT]: \n",
        "\n",
        "**Original training dataset**: Difference in mean outcomes between unprivileged and privileged groups = -0.169905 \n",
        "\n",
        "**Transformed training dataset**: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n",
        "\n",
        "**Explanation:**\n",
        "The credit decision pipeline is an example of pre-processing mitigation, because the bias is mitigated before the creation of the model. After executing the credit decision pipeline on the training dataset to detect bias in the protected attribute, age, it was determined that the privileged group (aged 25+ years) had a higher percentage of favorable results over the unprivileged group (under 25 years old) by 17%. This was determined by the mean_difference method on the BinaryLabelDataset class, which calculates the percentage of favorable outcomes for the privileged group, and subtracts it from the percentage of favorable outcomes for the unprivileged group. Because the bias was removed before the creation of the model.\n",
        "\n",
        "Once the age bias was identified (that the privileged group was getting 17% more positive outcomes from the training dataset), the reweighing algorithm was used to transform the training dataset to have more equity in positive outcomes on age for both the privileged and unprivileged groups. To determine the efficacy in removing bias of the transformation, the mean_difference method was again used on the transformed data, resulting in a difference in mean outcomes of 0. When comparing the mean outcomes from the original training dataset with that of the transformed training dataset, one can see from the difference in mean outcome values, -17% and 0% respectively, that the transformation effectively mitigated the 17% age bias to 0%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wYrz8pjis1RK"
      },
      "source": [
        "# **Using [Disparate Impact Remover](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html#aif360.algorithms.preprocessing.DisparateImpactRemover)** for detecting and removing bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hfEBpuDIUDY",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#Part 3\n",
        "\n",
        "#install all necessary libraries and modules\n",
        "!pip install aif360\n",
        "!pip install IPython\n",
        "!pip install BlackBoxAuditing\n",
        "!pip install wget\n",
        "\n",
        "\n",
        "#Retrieve Necessary Files \n",
        "import os\n",
        "os.chdir('/usr/local/lib/python3.6/dist-packages/aif360/data/raw/german')\n",
        "!wget -q\thttps://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
        "!wget -q\thttps://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc\n",
        "\n",
        "\n",
        "#import the Methods used from aif360\n",
        "from aif360.datasets import GermanDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing, DisparateImpactRemover\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "#Loading dataset\n",
        "dataset_orig = GermanDataset(\n",
        "    protected_attribute_names=['age'],           # this dataset also contains protected\n",
        "                                                 # attribute for \"sex\" which we do not\n",
        "                                                 # consider in this evaluation\n",
        "    privileged_classes=[lambda x: x >= 25],      # age >=25 is considered privileged\n",
        "    features_to_drop=['personal_status', 'sex'] # ignore sex-related attributes\n",
        ")\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "privileged_groups = [{'age': 1}]\n",
        "unprivileged_groups = [{'age': 0}]\n",
        "\n",
        "#Metric for original data set\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
        "\n",
        "\n",
        "#Using Disparate Impact Remover PreProcessor Method  \n",
        "\n",
        "#repair_level - Repair amount\n",
        "#0.0 = no repair \n",
        "#1.0 is full repair.\n",
        "DIR = DisparateImpactRemover(repair_level = 1.0)\n",
        "\n",
        "#Run a repairer on the non-protected features and return the transformed dataset.\n",
        "dataset_transf_DIR = DIR.fit_transform(dataset_orig_train)\n",
        "\n",
        "\n",
        "dataset_transf_DIR_train, dataset_transf_DR_test, = dataset_transf_DIR.split([0.7], shuffle=True)\n",
        "\n",
        "metric_DIR_train = BinaryLabelDatasetMetric(dataset_transf_DIR_train, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "display(Markdown(\"#### Transformed training dataset using Disparate Impact Remover PreProcessor Method\"))\n",
        "\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_DIR_train.mean_difference())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Id2EfMVhs__n"
      },
      "source": [
        "# Findings using Disparate Impact Remover"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBCZ4ciT2FPC",
        "colab_type": "text"
      },
      "source": [
        "The alternate method used was Disparate Impact Remover method. This is a different Pre-Processing technique which falls in the aif360 library. \n",
        "\n",
        "**Results:**\n",
        "\n",
        "**Original training dataset:**\n",
        "Difference in mean outcomes between unprivileged and privileged groups = -0.169905\n",
        "\n",
        "**Transformed training dataset using Disparate Impact Remover PreProcessor Method:**\n",
        "Difference in mean outcomes between unprivileged and privileged groups = -0.177184\n",
        "\n",
        "Using the Disparate Impact Remover method, the difference in mean outcome between unprivileged and privileged groups was -0.177184, which is unfortunately a **greater** difference than using the Reweighing method and even the original dataset. \n",
        "\n",
        "This demonstrates that although multiple techniques can be utilized for PreProcessing data, not all will be optimal.The reason being that the different methods have unique algorithmic approaches. Like the Reweighing method, the DIR method aims to increases group fairness but does this while prioritizing preserving rank-ordering within groups. In this case and dataset, this doesn't help as the labels and protected attributes aren't modified when the dataset is transformed. On the other hand, the Reweighing technique calculates the weight of each combination of a group and label to ensure fairness. \n",
        "\n",
        "Since rank order within groups in this dataset is based on age, the protected attribute and also the source of the bias itself, our results could be indicative of the preservation of the rank-order throughout the transformation. Mitigating the age bias was our goal, but because rank-order, which is determined by age, was preserved throughout the transformation, we actually see an increase in the mean difference of outcomes.\n",
        "\n",
        "For this dataset, the Reweighing method proved to be optimal as it transformed the dataset and mitigated the fairness to reach a difference in mean outcome of 0.00. If we were to do this again, we believe Optimized Pre-Processing would be the better alternate method, keeping the same protected attribute of age. This is especially apparent considering the size of this dataset. In hindsight, we see why DIR and LFR methods would be inefficient in mitigating fairness with this dataset. \n",
        "\n",
        " \n",
        " \n",
        "\n",
        "\n"
      ]
    }
  ]
}